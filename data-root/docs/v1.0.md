# DOCUMENTAÇÃO
**COMPONENTES:** Victor Bittencourt, Nestor Preissler, Erika Maschio, Matheus Neves e Bruno Kaue.  
**FERRAMENTAS-CHAVE:** Python, Jupyter Notebook, Pandas, Numpy, Seaborn, Scikit-Learn, Plotly e Matplotlib.

## S01T01: Pesquisar um Dataset
Durante o processo de seleção do dataset para o desenvolvimento do projeto, fomos avaliando diversas opções, que iam desde métricas financeiras até análises agronômicas, porém, contudo, escolhemos um dataset que abordava pesquisas sobre índices de depressão relacionados aos padrões de vida dos indivíduos.

**DATASET:** [https://www.kaggle.com/datasets/sumansharmadataworld/depression-surveydataset-for-analysis/data](https://www.kaggle.com/datasets/sumansharmadataworld/depression-surveydataset-for-analysis/data)

## S01T02: Descrição/Cenário/Objetivo
A definição do problema girou ao redor da seguinte narrativa:
“O PsyCaldeira, empresa (fictícia) que trabalha com tratamentos psicoterapêuticos e análise de índices de doenças psicológicas/neurodivergentes reportou baseado em uma suposição o aumento de possíveis casos de depressão e necessita de algum sistema que possa auxiliar a detecção e tratamento de dados categóricos em excesso baseado na arquitetura de armazenamento desses dados.”

Com isto em mente, cenário em questão (assim como o objetivo) iriam orbitar a necessidade de auxiliar no processo de detecção dos possíveis pacientes que se enquadram no perfil de pessoa com depressão e demonstrar um sistema efetivo/utilizável.

## S01T03: Versionar dataset e documentação
Sobre versionamento de código/dataset, o time preferiu uma aproximação mais tradicional quanto ao versionamento, onde existem 3 (três) branches principais, a main (ou branch raiz), a dev (branch de desenvolvimento) e as features (branches em que se adicionam funcionalidades ou se mostram novos progressos no desenvolvimento individual), o fluxo de imagem (entrada) se explica com facilidade seguindo o “diagrama” a seguir:  
main > dev > feature  
Neste caso, a imagem base é a main, da qual a dev vem a derivar e por último, as features se ramificam da dev.

Agora, quanto ao fluxo de atualização (saída), dá para explicar da seguinte forma:  
feature > dev > [PR] > main  
Nessa representação, a feature é a imagem base, da qual entra para dev por meio de um [PR] (Pull Request), para aí então ser aceita na main. No meio do processo “feature>dev” também existe um [PR], porém ele tem menor peso do que o [PR] “dev/main”.

Quanto à documentação, abrangiremos um fluxo de documentação “entre-tasks”, assim como também serão resumidos os acontecimentos como um todo.

## S02T01: Cálculos Estatístico Descritivos
[Bruno Kaue…] – não completo até o momento

## S02T02: Visualização Básica Gráfica
[Matheus Neves…] – não completo até o momento

## S02T03: Identificação e Tratamento de Dados Faltantes
Durante o processo de desenvolvimento, surgiram algumas observações quanto à análise inicial do dataset, haviam dados faltantes em colunas “context-relative”, assim como haviam informações faltantes por não-inputação. Sobre as colunas CR (“context-relative”) foi realizada a seguinte aproximação:

- **academic/work_pressure/satisfaction:** estas colunas eram complementares, por exemplo, quando o indivíduo possuía valores em academic (estudante) faltavam valores nas colunas work (trabalhador) e vice-versa, estes casos foram tratados por meio da criação de uma coluna intermediária, chamada “general_satisfaction/pressure”. O que evidentemente satisfazia a relação momentânea estudo/trabalho.
- **CGPA:** esta coluna representava o conceito médio acadêmico, faltando 86% das linhas presentes. A mesma foi retirada.

Após isto, foram analisadas as colunas com poucos valores faltantes, sendo ela a de profissão, da qual, por decisão comum, os valores faltantes seriam preenchidos com “Unemployed” para representar um possível desempregado.

## S02T04: Identificar e Tratar Outliers
Na tarefa citada em questão, foi possível notar após a exibição de alguns gráficos que o dataframe no seu estado completamente limpo e normalizado não contém nenhum outlier (notável) em quaisquer uma das colunas.

## S02T05: Gerar Relatório Inicial com Insights
[Bruno Kaue…] – não completo até o momento

## S03T01: Remover Duplicatas ou Inconsistências
Assim como na S02T03, aqui foi realizada a remoção de valores duplicados ou inconsistentes, o que pode ser definido por exemplo como: cidades, nome, nota acadêmica e os mesmos valores de relação estudo-trabalho.

## S03T02: Normalizar ou Padronizar Colunas
Ambos os processos de tratamento para futura aplicação a modelos de aprendizado de máquina foram realizados focando na padronalização dos valores, assim também como, na normalização. Colunas que podiam ser metrificadas como ‘dieta’, ou colunas categóricas como ‘está trabalhando’/’familiares com histórico psiquiátrico’ foram tratadas das devidas maneiras.

## S03T03: Transformar Variáveis Categóricas em Numéricas
Semelhante à entrega S03T02, realizamos os devidos tratamentos para evitar comparações de texto e permitir um maior entendimento ao modelo.

## S03T04: Não definida
Não temos.
