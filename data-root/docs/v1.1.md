# DOCUMENTAÇÃO
**COMPONENTES:** Victor Bittencourt, Nestor Preissler, Erika Maschio, Matheus Neves e Bruno Kaue.  
**FERRAMENTAS-CHAVE:** Python, Jupyter Notebook, Pandas, Numpy, Seaborn, Scikit-Learn, Plotly e Matplotlib.

## S01T01: Pesquisar um Dataset
Durante o processo de seleção do dataset para o desenvolvimento do projeto, fomos avaliando diversas opções, que iam desde métricas financeiras até análises agronômicas, porém, contudo, escolhemos um dataset que abordava pesquisas sobre índices de depressão relacionados aos padrões de vida dos indivíduos.

**DATASET:** [https://www.kaggle.com/datasets/sumansharmadataworld/depression-surveydataset-for-analysis/data](https://www.kaggle.com/datasets/sumansharmadataworld/depression-surveydataset-for-analysis/data)

## S01T02: Descrição/Cenário/Objetivo
A definição do problema girou ao redor da seguinte narrativa:
“O PsyCaldeira, empresa (fictícia) que trabalha com tratamentos psicoterapêuticos e análise de índices de doenças psicológicas/neurodivergentes reportou baseado em uma suposição o aumento de possíveis casos de depressão e necessita de algum sistema que possa auxiliar a detecção e tratamento de dados categóricos em excesso baseado na arquitetura de armazenamento desses dados.”

Com isto em mente, cenário em questão (assim como o objetivo) iriam orbitar a necessidade de auxiliar no processo de detecção dos possíveis pacientes que se enquadram no perfil de pessoa com depressão e demonstrar um sistema efetivo/utilizável.

## S01T03: Versionar dataset e documentação
Sobre versionamento de código/dataset, o time preferiu uma aproximação mais tradicional quanto ao versionamento, onde existem 3 (três) branches principais, a main (ou branch raiz), a dev (branch de desenvolvimento) e as features (branches em que se adicionam funcionalidades ou se mostram novos progressos no desenvolvimento individual), o fluxo de imagem (entrada) se explica com facilidade seguindo o “diagrama” a seguir:  
main > dev > feature  
Neste caso, a imagem base é a main, da qual a dev vem a derivar e por último, as features se ramificam da dev.

Agora, quanto ao fluxo de atualização (saída), dá para explicar da seguinte forma:  
feature > dev > [PR] > main  
Nessa representação, a feature é a imagem base, da qual entra para dev por meio de um [PR] (Pull Request), para aí então ser aceita na main. No meio do processo “feature>dev” também existe um [PR], porém ele tem menor peso do que o [PR] “dev/main”.

Quanto à documentação, abrangiremos um fluxo de documentação “entre-tasks”, assim como também serão resumidos os acontecimentos como um todo.

## S02T01: Cálculos Estatístico Descritivos
Conforme os dados retornados após a analise no dataset, foram possíveis as extrações de alguns dados importantes como: 
 -> Porcentagem de sujeitos que ja tiveram pensamentos suicidas = 48.9%
 -> Porcentagem de sujeitos que tem registros psicoterapeuticos na família = 48.7%
 -> Porcentagem de sujeitos com depressão diagnosticada = 17.8%
 -> Correlação de pensamentos suicidas com depressão = 28.0% 
 -> Covariancia entre depressão e pensamentos suicidas é pouco positiva
 -> Correlação entre depressão e histórico familiar = 2.0%
 -> Covariancia entre depressão e histórico familiar é quase nula 
 ...
 -> Idade geral do público = 39y (men -> 38/ woman-> 39)
 -> Mediana das idades = 39
 -> Idades que mais se repetem (dados modais) = 28 e 56

## S02T02: Visualização Básica Gráfica
 -> Possivel notar que a maioria das pessoas depressivas são desempregadas, em seguida de Designers Gráficos
 -> A depressão está frequentemente mais presente em pessoas jovens do qeu pessoas com mais idade
 -> Em todas as profissões, praticamente metade das pessoas entrevistadas já tiveram pensamentos suicídas

## S02T03: Identificação e Tratamento de Dados Faltantes
Durante o processo de desenvolvimento, surgiram algumas observações quanto à análise inicial do dataset, haviam dados faltantes em colunas “context-relative”, assim como haviam informações faltantes por não-inputação. Sobre as colunas CR (“context-relative”) foi realizada a seguinte aproximação:

- **academic/work_pressure/satisfaction:** estas colunas eram complementares, por exemplo, quando o indivíduo possuía valores em academic (estudante) faltavam valores nas colunas work (trabalhador) e vice-versa, estes casos foram tratados por meio da criação de uma coluna intermediária, chamada “general_satisfaction/pressure”. O que evidentemente satisfazia a relação momentânea estudo/trabalho.
- **CGPA:** esta coluna representava o conceito médio acadêmico, faltando 86% das linhas presentes. A mesma foi retirada.

Após isto, foram analisadas as colunas com poucos valores faltantes, sendo ela a de profissão, da qual, por decisão comum, os valores faltantes seriam preenchidos com “Unemployed” para representar um possível desempregado.

## S02T04: Identificar e Tratar Outliers
Na tarefa citada em questão, foi possível notar após a exibição de alguns gráficos que o dataframe no seu estado completamente limpo e normalizado não contém nenhum outlier (notável) em quaisquer uma das colunas.

## S02T05: Gerar Relatório Inicial com Insights
path -> projeto-gc/data-root/docs/EDA-Outliers.pdf

## S03T01: Remover Duplicatas ou Inconsistências
Assim como na S02T03, aqui foi realizada a remoção de valores duplicados ou inconsistentes, o que pode ser definido por exemplo como: cidades, nome, nota acadêmica e os mesmos valores de relação estudo-trabalho.

## S03T02: Normalizar ou Padronizar Colunas
Ambos os processos de tratamento para futura aplicação a modelos de aprendizado de máquina foram realizados focando na padronalização dos valores, assim também como, na normalização. Colunas que podiam ser metrificadas como ‘dieta’, ou colunas categóricas como ‘está trabalhando’/’familiares com histórico psiquiátrico’ foram tratadas das devidas maneiras.

## S03T03: Transformar Variáveis Categóricas em Numéricas
Semelhante à entrega S03T02, realizamos os devidos tratamentos para evitar comparações de texto e permitir um maior entendimento ao modelo.

## S03T04: Não definida
Não temos.

## S03T05: Criar Novas Features Relevantes para o Problema
[Realizar]

## S03T06: Salvar o Dataset limpo em ./data_root.csv
[Realizado, porém passível de mudança]

## S04T01: Dividir os Conjuntos de Teste/Treino
Após testes de distribuição a melhor forma de distribuir nos conjuntos encontrada foi: como 60% treino e 40% teste. O que possibilitou um resultado final perceptivelmente bom em outras tarefas desempenhadas a partir disso. 

## S04T02: Documentar Estratégia de Divisão
A divisão dos dados de teste e treino foi baseada nas features criadas e nas colunas que deviam ser utilizadas nas métricas de avaliação do modelo, levando em consideração o seguinte:
- Não se pode treinar com valores em excesso (overfitting), para evitar problemas de viés nas previsões;
- Não se pode treinar com outliers, para evitar viés de peso das classes;

No processo de divisão, foi escolhida a seguinte aproximação, um modelo 60/40 para evitar overfitting e também estimular o modelo a lidar com as mudanças, sendo mais flexível a possíveis valores diferentes
